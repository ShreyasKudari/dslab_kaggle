{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(df):\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    for x in df['f1']:\n",
    "        if x!=-1:\n",
    "            sum+=x\n",
    "            count+=1\n",
    "    avg = sum/count\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(model, X_train, X_valid,y_train, y_valid):\n",
    "    soft_probs = model.predict_proba(X_valid)\n",
    "    valid_auc = roc_auc_score(y_valid, soft_probs[:,1])\n",
    "    print(valid_auc)\n",
    "    train_preds = model.predict(X_train)\n",
    "    acc = accuracy_score(y_train, train_preds)\n",
    "    print(acc)\n",
    "    preds = model.predict(X_valid)\n",
    "    acc = accuracy_score(y_valid, preds)\n",
    "    print(acc)\n",
    "    print(classification_report(y_valid,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['feature_fraction_seed'] = seed\n",
    "        params['bagging_seed'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatboostWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_seed'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XgbWrapper(object):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 1538)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(xgb.DMatrix(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_final.csv\")\n",
    "df.describe()\n",
    "\n",
    "testdf = pd.read_csv(\"test_final.csv\")\n",
    "\n",
    "X_test = testdf.loc[:,'f1':'f24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loc[:,'Y']\n",
    "X = df.loc[:,'f1':'f24']\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size = 0.75, test_size = 0.25,random_state = 42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'colsample_bylevel': 0.8789900701708724,\n",
    " 'colsample_bynode': 0.9962857441883629,\n",
    " 'colsample_bytree': 0.7448296344616587,\n",
    " 'learning_rate': 0.036130784427853585,\n",
    " 'max_delta_step': 4.953440861348021,\n",
    " 'max_depth': 9,\n",
    " 'n_estimators': 980,\n",
    " 'subsample': 0.6002618478477451,\n",
    " 'base_score':0.475\n",
    "             }\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 990,\n",
    "    'learning_rate': 0.01600,\n",
    "    'max_depth': 8,\n",
    "#     'bootstrap_type': 'Bernoulli',\n",
    "    'subsample': 0.66322,\n",
    "    'early_stopping_rounds':30,\n",
    "    'eval_metric': 'AUC',\n",
    "    'verbose':100,\n",
    "    'one_hot_max_size':10,\n",
    "    'cat_features':[0,1,3,4,5,6,7,8,9,10,11,12,14,15,16,17,19,18,20,21,22,23]\n",
    "}\n",
    "\n",
    "# lightgbm_params = {\n",
    "#     'n_estimators':1850,\n",
    "#     'learning_rate':0.1,\n",
    "#     'num_leaves':123,\n",
    "#     'colsample_bytree':0.9,\n",
    "#     'subsample':0.3,\n",
    "#     'max_depth':-1,\n",
    "# #     'reg_alpha':0.1,\n",
    "# #     'reg_lambda':0.1,\n",
    "#     'min_split_gain':0.01,\n",
    "#     'min_child_weight':2    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 5\n",
    "SEED = 42\n",
    "NROWS = None\n",
    "ntrain = X.shape[0]\n",
    "ntest = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((folds, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X,y)):\n",
    "        x_tr = X.iloc[train_index]\n",
    "        y_tr = y.iloc[train_index]\n",
    "        x_te = X.iloc[test_index] #always training here\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(X_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:43:38] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:44:32] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:45:25] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:45:57] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:46:37] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0:\ttotal: 170ms\tremaining: 2m 48s\n",
      "100:\ttotal: 13.5s\tremaining: 1m 59s\n",
      "200:\ttotal: 31.6s\tremaining: 2m 3s\n",
      "300:\ttotal: 48.5s\tremaining: 1m 51s\n",
      "400:\ttotal: 1m 3s\tremaining: 1m 33s\n",
      "500:\ttotal: 1m 20s\tremaining: 1m 18s\n",
      "600:\ttotal: 1m 35s\tremaining: 1m 1s\n",
      "700:\ttotal: 1m 53s\tremaining: 46.7s\n",
      "800:\ttotal: 2m 4s\tremaining: 29.5s\n",
      "900:\ttotal: 2m 17s\tremaining: 13.6s\n",
      "989:\ttotal: 2m 29s\tremaining: 0us\n",
      "0:\ttotal: 80.5ms\tremaining: 1m 19s\n",
      "100:\ttotal: 9.03s\tremaining: 1m 19s\n",
      "200:\ttotal: 20.2s\tremaining: 1m 19s\n",
      "300:\ttotal: 32.2s\tremaining: 1m 13s\n",
      "400:\ttotal: 42s\tremaining: 1m 1s\n",
      "500:\ttotal: 53.1s\tremaining: 51.8s\n",
      "600:\ttotal: 1m 4s\tremaining: 41.6s\n",
      "700:\ttotal: 1m 15s\tremaining: 31s\n",
      "800:\ttotal: 1m 26s\tremaining: 20.5s\n",
      "900:\ttotal: 1m 39s\tremaining: 9.84s\n",
      "989:\ttotal: 1m 50s\tremaining: 0us\n",
      "0:\ttotal: 120ms\tremaining: 1m 58s\n",
      "100:\ttotal: 9.38s\tremaining: 1m 22s\n",
      "200:\ttotal: 21.8s\tremaining: 1m 25s\n",
      "300:\ttotal: 34.3s\tremaining: 1m 18s\n",
      "400:\ttotal: 45.9s\tremaining: 1m 7s\n",
      "500:\ttotal: 58.5s\tremaining: 57.1s\n",
      "600:\ttotal: 1m 10s\tremaining: 45.7s\n",
      "700:\ttotal: 1m 23s\tremaining: 34.2s\n",
      "800:\ttotal: 1m 35s\tremaining: 22.6s\n",
      "900:\ttotal: 1m 48s\tremaining: 10.7s\n",
      "989:\ttotal: 2m\tremaining: 0us\n",
      "0:\ttotal: 113ms\tremaining: 1m 51s\n",
      "100:\ttotal: 7.67s\tremaining: 1m 7s\n",
      "200:\ttotal: 20.1s\tremaining: 1m 19s\n",
      "300:\ttotal: 31.2s\tremaining: 1m 11s\n",
      "400:\ttotal: 41.2s\tremaining: 1m\n",
      "500:\ttotal: 52.6s\tremaining: 51.4s\n",
      "600:\ttotal: 1m 3s\tremaining: 41.2s\n",
      "700:\ttotal: 1m 14s\tremaining: 30.8s\n",
      "800:\ttotal: 1m 27s\tremaining: 20.6s\n",
      "900:\ttotal: 1m 40s\tremaining: 9.9s\n",
      "989:\ttotal: 1m 52s\tremaining: 0us\n",
      "0:\ttotal: 108ms\tremaining: 1m 46s\n",
      "100:\ttotal: 9.27s\tremaining: 1m 21s\n",
      "200:\ttotal: 20.8s\tremaining: 1m 21s\n",
      "300:\ttotal: 32s\tremaining: 1m 13s\n",
      "400:\ttotal: 43s\tremaining: 1m 3s\n",
      "500:\ttotal: 54s\tremaining: 52.7s\n",
      "600:\ttotal: 1m 4s\tremaining: 41.6s\n",
      "700:\ttotal: 1m 15s\tremaining: 31.3s\n",
      "800:\ttotal: 1m 27s\tremaining: 20.7s\n",
      "900:\ttotal: 1m 40s\tremaining: 9.92s\n",
      "989:\ttotal: 1m 51s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "xg = XgbWrapper(seed=SEED, params=xgb_params)\n",
    "cb = CatboostWrapper(clf= CatBoostClassifier, seed = SEED, params=catboost_params)\n",
    "# lg = LightGBMWrapper(clf = LGBMClassifier, seed = SEED, params = lightgbm_params)\n",
    "xg_oof_train, xg_oof_test = get_oof(xg)\n",
    "cb_oof_train, cb_oof_test = get_oof(cb)\n",
    "# lg_oof_train, lg_oof_test = get_oof(lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((xg_oof_train, cb_oof_train), axis=1)\n",
    "x_test = np.concatenate((xg_oof_test,cb_oof_test), axis=1)\n",
    "# x_train = cb_oof_train\n",
    "# x_test = cb_oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(x_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logistic_regression.predict_proba(x_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9053103324232585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.43      0.58       224\n",
      "           1       0.97      1.00      0.98      3872\n",
      "\n",
      "    accuracy                           0.97      4096\n",
      "   macro avg       0.92      0.71      0.78      4096\n",
      "weighted avg       0.96      0.97      0.96      4096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_auc = roc_auc_score(y_valid, preds)\n",
    "print(valid_auc)\n",
    "hard_preds = np.where(preds>0.5,1,0)\n",
    "print(classification_report(y_valid,hard_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub420 = {\"Id\":testdf['Id'],\"Y\":preds}\n",
    "sub420 = pd.DataFrame(data=sub420)\n",
    "sub420.to_csv(\"submissions/sub420.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
